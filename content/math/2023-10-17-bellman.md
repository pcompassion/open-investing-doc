+++
title = "Bellman equation (part2)"
author = ["littlehome"]
date = 2023-10-17
lastmod = 2023-10-17T16:25:39+09:00
draft = false
+++

## Definitions and Equations {#definitions-and-equations}

<https://chat.openai.com/share/e71eecee-4263-4dc6-842e-a0dee3af28b0>

this is part2 after [Bellman equation (part1)]({{< relref "2023-10-16-reinforcement-learning#bellman-equation--part1" >}})


### Definition of \\( G\_t \\) {#definition-of-g-t}

It represents the random variable of cumulative rewards at time \\( t \\). The cumulative rewards can be expressed as:

\begin{aligned}
     G\_t &= R\_{t+1} + \gamma R\_{t+2} + \gamma^2 R\_{t+3} + \dots \\\\
     G\_t &= R\_{t+1} + \gamma G\_{t+1}
\end{aligned}


### Definition of \\( V(s) \\) {#definition-of-v--s}

It is the expected cumulative rewards given state \\( s \\).
\\[
V(s) = \mathbb{E}[G\_t | S\_t = s]
\\]
Combining the above definitions, we get:
\\[
V(s) = \mathbb{E}[R\_{t+1} + \gamma G\_{t+1} | S\_t = s]
\\]


### The \\( Q \\) Function {#the-q-function}

If you further condition \\( V \\) on action, you get the \\( Q \\) function:
\\[
Q(s, a) = \mathbb{E}[R\_{t+1} + \gamma G\_{t+1} | S\_t = s, A\_t = a]
\\]


### Law of Total Probability {#law-of-total-probability}

Using the law of total probability:
\\[
V(s) = \sum\_{a} \pi(a|s) Q(s, a)
\\]
\\[
V(s) = \sum\_{a} \pi(a|s) \mathbb{E}[R\_{t+1} + \gamma G\_{t+1} | S\_t = s, A\_t = a]
\\]


### Additional Relations {#additional-relations}

\\[
G(t+1|s) = G(t|s)
\\]

Therefore, the relation can be expressed as:

\begin{aligned}
     Q(s, a) &= \mathbb{E}[R\_{t+1} + \gamma G\_{t+1} | S\_t = s, A\_t = a] \\\\
     Q(s, a) &= \mathbb{E}[R\_{t+1} + \gamma V(S\_{t+1}) | S\_t = s, A\_t = a] \\\\
     Q(s, a) &= \sum\_{s', r} p(s', r | s, a) [r + \gamma V(s')]
\end{aligned}

Resulting in:
\\[
V(s) = \sum\_a \pi(a|s) \sum\_{s', r} p(s', r | s, a) [r + \gamma V(s')]
\\]
