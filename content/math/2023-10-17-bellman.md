+++
title = "Bellman equation (part2)"
author = ["eugenekim"]
date = 2023-10-17
lastmod = 2023-11-28T01:28:33+09:00
draft = false
+++

## Definitions and Equations {#definitions-and-equations}

<https://chat.openai.com/share/e71eecee-4263-4dc6-842e-a0dee3af28b0>

this is part2 after [Bellman equation (part1)]({{< relref "2023-10-16-reinforcement-learning#bellman-equation--part1" >}})


### Definition of \\( G_t \\) {#definition-of--g-t}

It represents the random variable of cumulative rewards at time \\( t \\). The cumulative rewards can be expressed as:

\\[\\begin{aligned}
     G_t &amp;= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\\\
     G_t &amp;= R_{t+1} + \\gamma G_{t+1}
\\end{aligned} \\]


### Definition of \\( V(s) \\) {#definition-of--vs}

It is the expected cumulative rewards given state \\( s \\).
\\[
V(s) = \\mathbb{E}[G_t | S_t = s]
\\]
Combining the above definitions, we get:
\\[
V(s) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s]
\\]


### The \\( Q \\) Function {#the--q--function}

If you further condition \\( V \\) on action, you get the \\( Q \\) function:
\\[
Q(s, a) = \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]
\\]


### Law of Total Probability {#law-of-total-probability}

Using the law of total probability:
\\[
V(s) = \\sum_{a} \\pi(a|s) Q(s, a)
\\]
\\[
V(s) = \\sum_{a} \\pi(a|s) \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a]
\\]


### G's property {#g-s-property}

This states that G doesn't depend on t, and time series is supposed to be infinite.

\\[
G(t+1|s) = G(t|s)
\\]

Therefore, the relation can be expressed as:

\\[ \\begin{aligned}
     Q(s, a) &amp;= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} | S_t = s, A_t = a] \\\\
     Q(s, a) &amp;= \\mathbb{E}[R_{t+1} + \\gamma V(S_{t+1}) | S_t = s, A_t = a] \\\\
     Q(s, a) &amp;= \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]
\\end{aligned} \\]

Resulting in:
\\[
V(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]
\\]
