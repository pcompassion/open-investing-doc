+++
title = "Mathematical definition of distribution of wealth"
author = ["littlehome"]
date = 2023-10-20
lastmod = 2023-10-20T15:34:52+09:00
draft = false
+++

## Mathematical definition of distribution of wealth {#mathematical-definition-of-distribution-of-wealth}

-   Gini
    \\[ G = \frac{1}{2} \int\_{0}^{1} |F(y) - y| dy \\]
-   Theil Index
    \\[ T = \sum\_{i=1}^{n} p\_i \ln\left(\frac{p\_i}{q\_i}\right) \\]
-   Palma ratio, Atkinson Index, Wealth Concentration ..


## How is spread measured? {#how-is-spread-measured}


### MAD (mean absolute difference) {#mad--mean-absolute-difference}

You average absolute difference from mean.

Gini is essentially MAD.

\\[ G = E(|Y-\mu|) = \frac{1}{2} \int\_{0}^{1} |F(y) - y| dy \\]


### Variance (or STD) {#variance--or-std}

You average squared difference from mean. (then sqrt it)
\\[ \text{Var}(X) = E[(X - \mu)^2] \\]

In many cases, variance is used over MAD because it is mathematically nicer to work with.
But in measuring distribution of wealth, MAD is prefered because Variance is influced by the extreme (here the richest people's wealth is squared and averaged)


### Relative Entropy {#relative-entropy}

You look at the difference from what is considered to be a true distribution (or base distribution) to the observed distribution.

\\[ D\_{KL}(P||Q) = \sum\_x P(x) \log \frac{P(x)}{Q(x)} \\]

Theil index is a relative entropy (measures parity of distribution of wealth from distribution of population)


## How is variance related to relative entropy? {#how-is-variance-related-to-relative-entropy}


### Log-Likelihood {#log-likelihood}

\\[ \log \frac{Q(x)}{P(x)} \\]

It's a ratio of two probabilities, and logged.


### Log-Likelihood Ratio for Gaussian Distributions {#log-likelihood-ratio-for-gaussian-distributions}

Guassian:
\\[ P(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \\]

Log-likelyhood for two Gaussian if we assume there mean is the same:

\\[ \log \frac{P\_1(x)}{P\_2(x)} = -\frac{(x-\mu1)^2}{2\sigma\_1^2} + \frac{(x-\mu2)^2}{2\sigma\_2^2} + \frac{1}{2} \log \left( \frac{\sigma\_2^2}{\sigma\_1^2} \right) \\]

If let \\( \mu\_1 = \mu\_2 \\) :
\\[ \log \frac{P\_1(x)}{P\_2(x)} = -\frac{(x-\mu\_1)^2}{2\sigma^2} + \frac{(x-\mu\_2)^2}{2\sigma^2} \\]

If let \\( \sigma\_1 = \sigma\_2 \\) :
\\[ \log \frac{P\_1(x)}{P\_2(x)} = \frac{1}{2} \log \left( \frac{\sigma\_2^2}{\sigma\_1^2} \right) \\]

So \\( (x-\mu)^2 \\) plays the role of P(x), well of course that's the definition of Gaussian.

So we have not shown any relationship between log-likelihood ratio and variance in general distribution.
Inn retrospect, Gaussian is special in that it has (x-u)^2 in its pdf.
